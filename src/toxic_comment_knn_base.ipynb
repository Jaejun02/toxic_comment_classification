{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"2d0ba33d","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nrandom = 14 #Fix Random_State\n\nfrom nltk.stem import WordNetLemmatizer #Group Words with Same Form to Same Word\nimport nltk\n## Remove the Comments if they are Not Installed in your Environment.\n## nltk.download('stopwords')\n## nltk.download('wordnet')\n## nltk.download('omw-1.4')\nfrom nltk.corpus import stopwords #Stopwords refers to Words that are going to be Ignored\nimport string\nimport operator\nimport functools\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import balanced_accuracy_score,classification_report,confusion_matrix,f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-26T03:06:56.138823Z","iopub.execute_input":"2024-12-26T03:06:56.139187Z","iopub.status.idle":"2024-12-26T03:06:59.564611Z","shell.execute_reply.started":"2024-12-26T03:06:56.139155Z","shell.execute_reply":"2024-12-26T03:06:59.563204Z"}},"outputs":[],"execution_count":1},{"id":"56be68a8-696c-41db-a218-d6258475cb14","cell_type":"markdown","source":"# Import and Preprocess Datasets\n\n1. Read `train.csv` and `test.csv` and combine them into one dataset `D`.\n2. Force the target value into a binary value.\n3. Perform text preprocessing: Lower Casing, Counting number of words etc.\n4. Lemmatizing Strings.\n5. From `D` perform train-test-split to create training and testing dataset. (Step 1 and 5 was done to rearrange the size of 2 datasets.)","metadata":{}},{"id":"e3c74627","cell_type":"code","source":"## Dataset Paths\npath = '../data/'\ntrain_path = f'{path}train.csv'\ntest_path = f'{path}test.csv'\ntest_labels_path = f'{path}test_labels.csv'\ntrain_aug_path = f'{path}train_augmented_synonym.csv'\n\n## Reading Datasets\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)\ntest_labels = pd.read_csv(test_labels_path)[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\ntest = pd.concat([test, test_labels],axis=1)\ntest = test.loc[test['toxic'] != -1]\n\nD = pd.concat([train, test], axis=0)\n\n## Create New Dependent Variable - Malignant for D (train and test set only; augmented train dataset is already in the correct format)\ndef force_one(x):\n    if x > 1:\n        return 1\n    else:\n        return x\n\nD['malignant'] = D['toxic'] + D['obscene'] + D['threat'] + D['insult'] + D['identity_hate']\nD['malignant'] = D['malignant'].apply(lambda x: force_one(x))\nD = D[['id','comment_text','malignant']]\n\nD.head()\n\n## Text Pre-Processing - D\n\n#### Make Strings to Lower Case\nD['comment_text'] = D['comment_text'].str.lower()\n\n#### Keep Track of String's Original Length\nD['length'] = D['comment_text'].str.len()\n\n#### Replace Email Address with 'email'\nD['comment_text'] = D['comment_text'].str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','email')\n\n#### Replace Website Address with 'website'\nD['comment_text'] = D['comment_text'].str.replace(r'^http[s]{0,1}\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$','website')\n\n#### Replace Website Address with 'phonenumber'\nD['comment_text'] = D['comment_text'].str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','phonenumber') # Note, might contain a random 10 digit number.\n\n#### Replace Numbers with 'numbrs'\nD['comment_text'] = D['comment_text'].str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n\n#### Special Punctuations are Replaced Explicitly.\nD['comment_text'] = D['comment_text'].str.replace(r'!',' exclamationmark')\nD['comment_text'] = D['comment_text'].str.replace(r'\\?',' questionmark')\nD['comment_text'] = D['comment_text'].str.replace(r'\\.{1}',' periodmark')\nD['comment_text'] = D['comment_text'].str.replace(r'\\.{2,}',' ellipsismark')\nD['comment_text'] = D['comment_text'].str.replace(r'Â£|\\$', ' dollers')\n\n## Removing Leftover Punctuations\ndef remove_punct(text):\n    p_free=\"\".join([i for i in text if i not in string.punctuation])\n    return p_free\nstop_words = set(stopwords.words('english'))\n\nD['comment_text'] = D['comment_text'].apply(lambda x:remove_punct(x))\n\nD['comment_text'] = D['comment_text'].apply(lambda x:remove_punct(x))\nD['comment_text'] = D['comment_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n\nlemmatizer = WordNetLemmatizer()\nD['comment_text'] = D['comment_text'].apply(lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split()))\n\nX = D['comment_text']\ny = D['malignant']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random)","metadata":{},"outputs":[],"execution_count":2},{"id":"74a762f9-bf6e-4e31-8991-db92d7e1e20c","cell_type":"markdown","source":"Since having an unbalanced dataset might cause the problems such as insufficient learning about lesser target value etc., a balanced dataset was created via undersampling. Since it decreases the data size, it might not help, but it was tried.","metadata":{}},{"id":"7c8a5f19","cell_type":"code","source":"#Prepare balanced dataset obtained via undersampling\n\nnonmalig = X_train[y_train == 0]\nnonmalig_y = y_train[y_train == 0]\nnum_nonmalig = len(nonmalig)\nmalig = X_train[y_train == 1]\nmalig_y = y_train[y_train == 1]\nnum_malig = len(malig)\n\n#chosen_idx = np.random.choice(num_nonmalig,replace=False,size=num_malig)\nchosen_idx_path = 'data/chosen_idx.csv'\nchosen_idx = pd.read_csv(chosen_idx_path)\nchosen_idx = np.array(chosen_idx['0'])\nchosen_nonmalig = nonmalig.iloc[chosen_idx]\nchosen_nonmalig_y = np.array(y_train)[chosen_idx]\n\nX_train_un = pd.concat([chosen_nonmalig, malig],axis=0)\ny_train_un = np.concatenate((chosen_nonmalig_y, malig_y), axis=None)\n\nundersampled_num = len(y_train_un)\n\nmix_idx_path = f'{path}mix_idx.csv'\nmix_idx = pd.read_csv(mix_idx_path)\nmix_idx = np.array(mix_idx['0'])\n\nX_train_un = X_train_un.iloc[mix_idx]\ny_train_un = y_train_un[mix_idx]\n","metadata":{},"outputs":[],"execution_count":5},{"id":"f9604c77-a63c-46e3-b2a6-d9f986b70cdc","cell_type":"markdown","source":"Vertorizing list of words to train Decision Tree Model using `TfidfVectorizer`.","metadata":{}},{"id":"6dc25300","cell_type":"code","source":"# Changing Words into Vector - Something Like One Hot Encoding\nword_vectorize = TfidfVectorizer(max_features = 20000, stop_words='english')\nX_train = word_vectorize.fit_transform(X_train)\nX_test_copy = X_test.copy()\nX_test = word_vectorize.transform(X_test)\n\nword_vectorize = TfidfVectorizer(max_features = 20000, stop_words='english')\nX_train_un = word_vectorize.fit_transform(X_train_un)\nX_test_un = word_vectorize.transform(X_test_copy)\n\nword_vectorize = TfidfVectorizer(max_features = 20000, stop_words='english')\nX_train_aug = word_vectorize.fit_transform(train_aug)\nX_test_aug = word_vectorize.transform(X_test_copy)\n","metadata":{},"outputs":[],"execution_count":6},{"id":"25d8ab8b-e113-49bd-aede-bc5e6b37dc79","cell_type":"markdown","source":"# Training K-Nearest Neighbors Model for Baseline","metadata":{}},{"id":"ff19d1d9","cell_type":"markdown","source":"## Using Original Unbalanced Dataset","metadata":{}},{"id":"f755a52f","cell_type":"code","source":"# KNN for Unbalanced Dataset\n\nknn = KNeighborsClassifier()\n\nparams = {\n    'n_neighbors': [10,50,100,200,300,400], \n    'weights' : ['uniform', 'distance'], \n    'metric' : ['euclidean','manhattan', 'cosine']}\n\ngrid_search  = GridSearchCV(estimator=knn,\n                           cv=5,\n                           param_grid=params,\n                           n_jobs=-1,\n                           verbose=10,\n                           scoring=\"f1_macro\")\n\ngrid_search.fit(X_train, y_train)\nprint(grid_search.best_estimator_)\nprint(grid_search.best_params_)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f68f8143","cell_type":"code","source":"# results = pd.DataFrame(grid_search.cv_results_).sort_values('rank_test_score')\n# results.to_csv('KNNresults.csv')","metadata":{},"outputs":[],"execution_count":9},{"id":"b742c037-2fd7-4199-8a72-29c9beb1b269","cell_type":"markdown","source":"The best model parameter found by GridSearch for KNN and Unbalanced Dataset is \n* n_neighbors = 10\n* weights = distance\n* metric = cosine\n\nThe Final Test Score run on test set was:\n* Accuracy: 0.93\n* F1-Macro Score: 0.7728","metadata":{}},{"id":"b26d60fd","cell_type":"code","source":"#results using top parameters (unbalanced)\nprint(grid_search.best_params_)\nclf = grid_search.best_estimator_\nclf.fit(X_train, y_train)\ny_pred_knn = clf.predict(X_test)\nprint('[KNN] f1 macro score of Unbalanced Dataset is {}'.format(f1_score(y_test, y_pred_knn, average = 'macro')))\nprint(confusion_matrix(y_test,y_pred_knn))\nprint(classification_report(y_test,y_pred_knn))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"dea6e832","cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ncv = 10\nskf = StratifiedKFold(n_splits=cv)\nscores = []\nfor i, (train_index, test_index) in enumerate(skf.split(X_train,y_train)):\n    X_t = X_train[train_index]\n    X_v = X_train[test_index]\n    y_t = y_train.iloc[train_index]\n    y_v = y_train.iloc[test_index]\n    clf = KNeighborsClassifier(metric = 'cosine', n_neighbors = 10, weights = 'distance') #using best parameters\n    clf.fit(X_t, y_t)\n    y_pred = clf.predict(X_v)\n    score = f1_score(y_v, y_pred, average = 'macro')\n    scores.append(score)\n\nknn_mean = np.mean(scores)\nknn_std = np.std(scores)\nknn_ci = [knn_mean - 3*knn_std, knn_mean + 3*knn_std] ## Includes 99.7% numbers around est. mean.\n\nprint(\"Mean CV Score: {}\".format(knn_mean))\nprint(\"Standard Deviation CV Score: {}\".format(knn_std))\nprint(\"99.7% Confidence Interval: {}\".format(knn_ci))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"043318be","cell_type":"markdown","source":"## Using Undersampled Dataset","metadata":{}},{"id":"474c5098","cell_type":"code","source":"# KNN for Undersampled Balanced Dataset\n\nknn = KNeighborsClassifier()\n\nparams = {\n    'n_neighbors': [10,50,100,200,300,400], #in intervals of 10\n    'weights' : ['uniform', 'distance'], #uniform equivalent to no weights\n    'metric' : ['euclidean','manhattan', 'cosine']}\n\ngrid_search  = GridSearchCV(estimator=knn,\n                           cv = 5,\n                           param_grid=params,\n                           verbose=10,\n                           scoring=\"f1_macro\")\n\ngrid_search.fit(X_train_un, y_train_un)\nprint(grid_search.best_estimator_)\nprint(grid_search.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"id":"c0635877","cell_type":"code","source":"# results = pd.DataFrame(grid_search.cv_results_).sort_values('rank_test_score')\n# results.to_csv('KNNUndersampledResults.csv')","metadata":{},"outputs":[],"execution_count":null},{"id":"a051b127-f395-40e1-a37d-7a7714f1532f","cell_type":"markdown","source":"The best model parameter found by GridSearch for KNN and Undersampled Dataset is \n* n_neighbors = 200\n* weights = distance\n* metric = euclidean\n\nThe Final Test Score run on test set was:\n* Accuracy: 0.93\n* F1-Macro Score: 0.7806","metadata":{}},{"id":"15a542bb","cell_type":"code","source":"#results using top parameters (undersampled balanced)\n\nprint(grid_search.best_params_)\nclf = grid_search.best_estimator_\nclf.fit(X_train_un, y_train_un)\ny_bal_pred_knn = clf.predict(X_test_un)\nprint('[DT] f1 macro score of Undersampled Balanced Dataset is {}'.format(f1_score(y_test, y_bal_pred_knn, average = 'macro')))\nprint(confusion_matrix(y_test,y_bal_pred_knn))\nprint(classification_report(y_test,y_bal_pred_knn))","metadata":{},"outputs":[],"execution_count":null},{"id":"9fe7fb42","cell_type":"code","source":"cv = 10\nskf = StratifiedKFold(n_splits=cv)\nscores = []\nfor i, (train_index, test_index) in enumerate(skf.split(X_train_un,y_train_un)):\n    X_t = X_train_un[train_index]\n    X_v = X_train_un[test_index]\n    y_t = y_train_un[train_index]\n    y_v = y_train_un[test_index]\n    clf = KNeighborsClassifier(metric = 'euclidean', n_neighbors = 200, weights = 'distance')\n    clf.fit(X_t, y_t)\n    y_pred = clf.predict(X_v)\n    score = f1_score(y_v, y_pred, average = 'macro')\n    scores.append(score)","metadata":{},"outputs":[],"execution_count":null},{"id":"04369a18","cell_type":"code","source":"knn_un_mean = np.mean(scores)\nknn_un_std = np.std(scores)\nknn_un_ci = [knn_un_mean - 3*knn_un_std, knn_un_mean + 3*knn_un_std] ## Includes 99.7% numbers around est. mean.","metadata":{},"outputs":[],"execution_count":null},{"id":"59bf53cd","cell_type":"code","source":"print(\"Mean CV Score: {}\".format(knn_un_mean))\nprint(\"Standard Deviation CV Score: {}\".format(knn_un_std))\nprint(\"99.7% Confidence Interval: {}\".format(knn_un_ci))","metadata":{},"outputs":[],"execution_count":null}]}